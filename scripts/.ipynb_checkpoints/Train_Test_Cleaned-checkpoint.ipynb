{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df76555e-c72f-48d1-ad92-fbebd932e8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, operator, csv, random, pickle,re\n",
    "\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from spacy.symbols import nsubj, VERB, dobj\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27b5d4c-261d-4706-8dcf-625a1c2746a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'comment_text'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce5941b0-d65c-44a2-af60-3c0f2a93ebd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    \n",
    "data_folder = \"../input/\"\n",
    "pretrained_folder = \"../input/\"\n",
    "train_filepath = data_folder + \"train.csv.zip\"\n",
    "test_filepath = data_folder + \"test.csv.zip\"\n",
    "\n",
    "#path to a submission\n",
    "submission_path =  data_folder + \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cfa4ff-467d-4db5-afd9-9d2a0e119cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23941\n",
      "65022\n",
      "17336\n",
      "47\n",
      "49\n",
      "17336\n"
     ]
    }
   ],
   "source": [
    "#paths to pretrained dictionaries\n",
    "hyphens_filepath = \"../input/cleaning-dictionaries/hyphens_dictionary.bin\"\n",
    "misspellings_filepath = \"../input/cleaning-dictionaries/misspellings_all_dictionary.bin\"\n",
    "merged_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n",
    "\n",
    "toxic_words_filepath = \"../input/cleaning-dictionaries/toxic_words.bin\"\n",
    "asterisk_words_filepath = \"../input/cleaning-dictionaries/asterisk_words.bin\"\n",
    "fasttext_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n",
    "\n",
    "with open(hyphens_filepath, mode='rb') as file: hyphens_dict = pickle.load(file)\n",
    "with open(misspellings_filepath, mode='rb') as file: misspellings_dict = pickle.load(file)\n",
    "with open(merged_filepath, mode='rb') as file: merged_dict = pickle.load(file)\n",
    "with open(toxic_words_filepath, mode='rb') as file: toxic_words = pickle.load(file)\n",
    "with open(asterisk_words_filepath, mode='rb') as file: asterisk_words = pickle.load(file)\n",
    "with open(fasttext_filepath, mode='rb') as file: fasttext_misspelings = pickle.load(file)\n",
    "    \n",
    "print(len(hyphens_dict))\n",
    "print(len(misspellings_dict))\n",
    "print(len(merged_dict)) \n",
    "print(len(toxic_words))\n",
    "print(len(asterisk_words))\n",
    "print(len(fasttext_misspelings)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79bcb3fc-a401-4ca8-9d2d-a5bada50f667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_samples_count = 149571\n",
    "validation_samples_count = 10000\n",
    "\n",
    "length_threshold = 20000 #We are going to truncate a comment if its length > threshold\n",
    "word_count_threshold = 900 #We are going to truncate a comment if it has more words than our threshold\n",
    "words_limit = 310000\n",
    "\n",
    "#We will filter all characters except alphabet characters and some punctuation\n",
    "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_characters_ext = valid_characters + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_set = set(x for x in valid_characters)\n",
    "valid_set_ext = set(x for x in valid_characters_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4b86b9-f2bb-42e4-bb58-dd188d06cb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_patterns = [\n",
    "    (r'(W|w)on\\'t', r'will not'),\n",
    "    (r'(C|c)an\\'t', r'can not'),\n",
    "    (r'(I|i)\\'m', r'i am'),\n",
    "    (r'(A|a)in\\'t', r'is not'),\n",
    "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def split_word(word, toxic_words):\n",
    "    if word == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    lower = word.lower()\n",
    "    for toxic_word in toxic_words:\n",
    "        start = lower.find(toxic_word)\n",
    "        if start >= 0:\n",
    "            end = start + len(toxic_word)\n",
    "            result = \" \".join([word[0:start], word[start:end], split_word(word[end:], toxic_words)])\n",
    "            return result.replace(\"  \", \" \").strip()\n",
    "    return word\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "def word_tokenize(sentence):\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    sentence = sentence.replace(\"@\", \"a\")    \n",
    "    sentence = sentence.replace(\"!\", \" ! \")\n",
    "    sentence = sentence.replace(\"?\", \" ? \")\n",
    "    \n",
    "    return tknzr.tokenize(sentence)\n",
    "\n",
    "def replace_url(word):\n",
    "    if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n",
    "        return \"\"\n",
    "    return word\n",
    "\n",
    "def normalize_by_dictionary(normalized_word, dictionary):\n",
    "    result = []\n",
    "    for word in normalized_word.split():\n",
    "        if word == word.upper():\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()].upper())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()])\n",
    "            else:\n",
    "                result.append(word)\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6454af0-5c29-49a2-bab2-d75cdf97fe01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tunguz/Library/Python/3.9/lib/python/site-packages/spacy/util.py:918: UserWarning: [W094] Model 'en_core_web_sm' (2.2.0) specifies an under-constrained spaCy version requirement: >=2.2.0. This can lead to compatibility problems with older versions, or as new spaCy versions are released, because the model may say it's compatible when it's not. Consider changing the \"spacy_version\" in your meta.json to a version range, with a lower and upper pin. For example: >=3.7.5,<3.8.0\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E053] Could not read config file from /Users/tunguz/Library/Python/3.9/lib/python/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43men_core_web_sm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_comment\u001b[39m(comment):\n\u001b[1;32m      4\u001b[0m     comment \u001b[38;5;241m=\u001b[39m unidecode(comment)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/en_core_web_sm/__init__.py:12\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_init_py\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;18;43m__file__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[0;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/util.py:538\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    536\u001b[0m config_path \u001b[38;5;241m=\u001b[39m model_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.cfg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 538\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[1;32m    540\u001b[0m     config,\n\u001b[1;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    546\u001b[0m )\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/spacy/util.py:714\u001b[0m, in \u001b[0;36mload_config\u001b[0;34m(path, overrides, interpolate)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 714\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE053\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mconfig_path, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig file\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfrom_disk(\n\u001b[1;32m    716\u001b[0m         config_path, overrides\u001b[38;5;241m=\u001b[39moverrides, interpolate\u001b[38;5;241m=\u001b[39minterpolate\n\u001b[1;32m    717\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: [E053] Could not read config file from /Users/tunguz/Library/Python/3.9/lib/python/site-packages/en_core_web_sm/en_core_web_sm-2.2.0/config.cfg"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def normalize_comment(comment):\n",
    "    comment = unidecode(comment)\n",
    "    comment = comment[:length_threshold]\n",
    "    \n",
    "    normalized_words = []\n",
    "    \n",
    "    for w in astericks_words:\n",
    "        if w[0] in comment:\n",
    "            comment = comment.replace(w[0], w[1])\n",
    "        if w[0].upper() in comment:\n",
    "            comment = comment.replace(w[0].upper(), w[1].upper())\n",
    "    \n",
    "    for word in word_tokenize(comment):\n",
    "        #for (pattern, repl) in patterns:\n",
    "        #    word = re.sub(pattern, repl, word)\n",
    "\n",
    "        if word == \".\" or word == \",\":\n",
    "            normalized_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        word = replace_url(word)\n",
    "        if word.count(\".\") == 1:\n",
    "            word = word.replace(\".\", \" \")\n",
    "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
    "                    \n",
    "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
    "        #If so, split this word by swear and non-swear part.\n",
    "        normalized_word = split_word(filtered_word, toxic_words)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, misspellings_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
    "\n",
    "\n",
    "        normalized_words.append(normalized_word)\n",
    "        \n",
    "    normalized_comment = \" \".join(normalized_words)\n",
    "    \n",
    "    result = []\n",
    "    for word in normalized_comment.split():\n",
    "        if word.upper() == word:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())\n",
    "    \n",
    "    #apparently, people on wikipedia love to talk about sockpuppets :-)\n",
    "    result = \" \".join(result)\n",
    "    if \"sock puppet\" in result:\n",
    "        result = result.replace(\"sock puppet\", \"sockpuppet\")\n",
    "    \n",
    "    if \"SOCK PUPPET\" in result:\n",
    "        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85edd417-812c-4d2b-8962-dd28a73220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386fa44-e552-458d-80c2-0e0bd660368e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
