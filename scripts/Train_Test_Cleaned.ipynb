{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df76555e-c72f-48d1-ad92-fbebd932e8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, operator, csv, random, pickle,re\n",
    "\n",
    "import gc\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#from spacy.symbols import nsubj, VERB, dobj\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27b5d4c-261d-4706-8dcf-625a1c2746a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COLUMN = 'comment_text'\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
    "submission = pd.read_csv(\"../input/sample_submission.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5941b0-d65c-44a2-af60-3c0f2a93ebd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categories = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    \n",
    "data_folder = \"../input/\"\n",
    "pretrained_folder = \"../input/\"\n",
    "train_filepath = data_folder + \"train.csv.zip\"\n",
    "test_filepath = data_folder + \"test.csv.zip\"\n",
    "\n",
    "#path to a submission\n",
    "submission_path =  data_folder + \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cfa4ff-467d-4db5-afd9-9d2a0e119cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23941\n",
      "65022\n",
      "17336\n",
      "47\n",
      "49\n",
      "17336\n"
     ]
    }
   ],
   "source": [
    "#paths to pretrained dictionaries\n",
    "hyphens_filepath = \"../input/cleaning-dictionaries/hyphens_dictionary.bin\"\n",
    "misspellings_filepath = \"../input/cleaning-dictionaries/misspellings_all_dictionary.bin\"\n",
    "merged_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n",
    "\n",
    "toxic_words_filepath = \"../input/cleaning-dictionaries/toxic_words.bin\"\n",
    "asterisk_words_filepath = \"../input/cleaning-dictionaries/asterisk_words.bin\"\n",
    "fasttext_filepath = \"../input/cleaning-dictionaries/merged_all_dictionary.bin\"\n",
    "\n",
    "with open(hyphens_filepath, mode='rb') as file: hyphens_dict = pickle.load(file)\n",
    "with open(misspellings_filepath, mode='rb') as file: misspellings_dict = pickle.load(file)\n",
    "with open(merged_filepath, mode='rb') as file: merged_dict = pickle.load(file)\n",
    "with open(toxic_words_filepath, mode='rb') as file: toxic_words = pickle.load(file)\n",
    "with open(asterisk_words_filepath, mode='rb') as file: asterisk_words = pickle.load(file)\n",
    "with open(fasttext_filepath, mode='rb') as file: fasttext_misspelings = pickle.load(file)\n",
    "    \n",
    "print(len(hyphens_dict))\n",
    "print(len(misspellings_dict))\n",
    "print(len(merged_dict)) \n",
    "print(len(toxic_words))\n",
    "print(len(asterisk_words))\n",
    "print(len(fasttext_misspelings)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79bcb3fc-a401-4ca8-9d2d-a5bada50f667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_samples_count = 149571\n",
    "validation_samples_count = 10000\n",
    "\n",
    "length_threshold = 20000 #We are going to truncate a comment if its length > threshold\n",
    "word_count_threshold = 900 #We are going to truncate a comment if it has more words than our threshold\n",
    "words_limit = 310000\n",
    "\n",
    "#We will filter all characters except alphabet characters and some punctuation\n",
    "valid_characters = \" \" + \"@$\" + \"'!?-\" + \"abcdefghijklmnopqrstuvwxyz\" + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_characters_ext = valid_characters + \"abcdefghijklmnopqrstuvwxyz\".upper()\n",
    "valid_set = set(x for x in valid_characters)\n",
    "valid_set_ext = set(x for x in valid_characters_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4b86b9-f2bb-42e4-bb58-dd188d06cb89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_patterns = [\n",
    "    (r'(W|w)on\\'t', r'will not'),\n",
    "    (r'(C|c)an\\'t', r'can not'),\n",
    "    (r'(I|i)\\'m', r'i am'),\n",
    "    (r'(A|a)in\\'t', r'is not'),\n",
    "    (r'(\\w+)\\'ll', r'\\g<1> will'),\n",
    "    (r'(\\w+)n\\'t', r'\\g<1> not'),\n",
    "    (r'(\\w+)\\'ve', r'\\g<1> have'),\n",
    "    (r'(\\w+)\\'s', r'\\g<1> is'),\n",
    "    (r'(\\w+)\\'re', r'\\g<1> are'),\n",
    "    (r'(\\w+)\\'d', r'\\g<1> would'),\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "def split_word(word, toxic_words):\n",
    "    if word == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    lower = word.lower()\n",
    "    for toxic_word in toxic_words:\n",
    "        start = lower.find(toxic_word)\n",
    "        if start >= 0:\n",
    "            end = start + len(toxic_word)\n",
    "            result = \" \".join([word[0:start], word[start:end], split_word(word[end:], toxic_words)])\n",
    "            return result.replace(\"  \", \" \").strip()\n",
    "    return word\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=True)\n",
    "def word_tokenize(sentence):\n",
    "    sentence = sentence.replace(\"$\", \"s\")\n",
    "    sentence = sentence.replace(\"@\", \"a\")    \n",
    "    sentence = sentence.replace(\"!\", \" ! \")\n",
    "    sentence = sentence.replace(\"?\", \" ? \")\n",
    "    \n",
    "    return tknzr.tokenize(sentence)\n",
    "\n",
    "def replace_url(word):\n",
    "    if \"http://\" in word or \"www.\" in word or \"https://\" in word or \"wikipedia.org\" in word:\n",
    "        return \"\"\n",
    "    return word\n",
    "\n",
    "def normalize_by_dictionary(normalized_word, dictionary):\n",
    "    result = []\n",
    "    for word in normalized_word.split():\n",
    "        if word == word.upper():\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()].upper())\n",
    "            else:\n",
    "                result.append(word)\n",
    "        else:\n",
    "            if word.lower() in dictionary:\n",
    "                result.append(dictionary[word.lower()])\n",
    "            else:\n",
    "                result.append(word)\n",
    "    \n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6454af0-5c29-49a2-bab2-d75cdf97fe01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def normalize_comment(comment):\n",
    "    comment = unidecode(comment)\n",
    "    comment = comment[:length_threshold]\n",
    "    \n",
    "    normalized_words = []\n",
    "    \n",
    "    for w in asterisk_words:\n",
    "        if w[0] in comment:\n",
    "            comment = comment.replace(w[0], w[1])\n",
    "        if w[0].upper() in comment:\n",
    "            comment = comment.replace(w[0].upper(), w[1].upper())\n",
    "    \n",
    "    for word in word_tokenize(comment):\n",
    "        #for (pattern, repl) in patterns:\n",
    "        #    word = re.sub(pattern, repl, word)\n",
    "\n",
    "        if word == \".\" or word == \",\":\n",
    "            normalized_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        word = replace_url(word)\n",
    "        if word.count(\".\") == 1:\n",
    "            word = word.replace(\".\", \" \")\n",
    "        filtered_word = \"\".join([x for x in word if x in valid_set])\n",
    "                    \n",
    "        #Kind of hack: for every word check if it has a toxic word as a part of it\n",
    "        #If so, split this word by swear and non-swear part.\n",
    "        normalized_word = split_word(filtered_word, toxic_words)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, hyphens_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, merged_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, misspellings_dict)\n",
    "        normalized_word = normalize_by_dictionary(normalized_word, fasttext_misspelings)\n",
    "\n",
    "\n",
    "        normalized_words.append(normalized_word)\n",
    "        \n",
    "    normalized_comment = \" \".join(normalized_words)\n",
    "    \n",
    "    result = []\n",
    "    for word in normalized_comment.split():\n",
    "        if word.upper() == word:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            result.append(word.lower())\n",
    "    \n",
    "    #apparently, people on wikipedia love to talk about sockpuppets :-)\n",
    "    result = \" \".join(result)\n",
    "    if \"sock puppet\" in result:\n",
    "        result = result.replace(\"sock puppet\", \"sockpuppet\")\n",
    "    \n",
    "    if \"SOCK PUPPET\" in result:\n",
    "        result = result.replace(\"SOCK PUPPET\", \"SOCKPUPPET\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85edd417-812c-4d2b-8962-dd28a73220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_files(train_filepath, test_filepath):\n",
    "    #read train data\n",
    "    train = pd.read_csv(train_filepath)\n",
    "\n",
    "\n",
    "    labels = train[categories].values\n",
    "    \n",
    "    #read test data\n",
    "    test = pd.read_csv(test_filepath)\n",
    "\n",
    "    test_comments = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "    #normalize comments\n",
    "    np_normalize = np.vectorize(normalize_comment)\n",
    "    comments = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "    normalized_comments = np_normalize(comments)\n",
    "    del comments\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    comments = test[\"comment_text\"].fillna(\"_na_\").values\n",
    "    normalized_test_comments = np_normalize(test_comments)\n",
    "    del comments\n",
    "    gc.collect()\n",
    "       \n",
    "\n",
    "    print('Shape of data tensor:', normalized_comments.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "    print('Shape of test data tensor:', normalized_test_comments.shape)\n",
    "    \n",
    "    return (labels, normalized_comments, normalized_test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9386fa44-e552-458d-80c2-0e0bd660368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571,)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test data tensor: (153164,)\n",
      "CPU times: user 2min 45s, sys: 1.59 s, total: 2min 47s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labels, x_train, x_test = read_data_files(train_filepath, test_filepath) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6612ed93-0784-4270-af89-14f226b42fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 ms, sys: 2.56 s, total: 2.56 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.save(\"../cleaned_data/lables\", labels)\n",
    "np.save(\"../cleaned_data/x_train\", x_train)\n",
    "np.save(\"../cleaned_data/x_test\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2fcf0-29fb-4d1a-b155-2a5669740e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc42d70-fd98-4376-a386-9cbf6cd9ee10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
